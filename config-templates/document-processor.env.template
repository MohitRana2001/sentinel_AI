# Environment Configuration Template for Document Processor VM

# ============================================
# Document Processor VM Configuration
# ============================================
# Copy this file to /opt/sentinel_ai/backend/.env
# Replace all <PLACEHOLDER> values with your actual configuration

# ------------------
# Environment Settings
# ------------------
ENV=production
DEBUG=False

# ------------------
# Redis Configuration
# ------------------
# Redis server coordinates work distribution between services
# This should point to your centralized Redis server

REDIS_HOST=<REDIS_SERVER_IP>
# Example: REDIS_HOST=10.0.2.10
# Example: REDIS_HOST=redis.internal.example.com

REDIS_PORT=6379

REDIS_DB=0

# Redis password (leave empty if no password is set)
REDIS_PASSWORD=<REDIS_PASSWORD>
# Example: REDIS_PASSWORD=strong_redis_password_123
# Example: REDIS_PASSWORD=

# ------------------
# Redis Queue Names
# ------------------
# These MUST match the main application configuration
# Do NOT change these unless you change them everywhere

REDIS_QUEUE_DOCUMENT=document_queue
REDIS_QUEUE_GRAPH=graph_queue
REDIS_QUEUE_AUDIO=audio_queue
REDIS_QUEUE_VIDEO=video_queue

# ------------------
# Database Configuration (AlloyDB/PostgreSQL)
# ------------------
# Database stores document metadata, embeddings, and chunks
# This should point to your centralized database server

ALLOYDB_HOST=<ALLOYDB_SERVER_IP>
# Example: ALLOYDB_HOST=10.0.3.10
# Example: ALLOYDB_HOST=alloydb.internal.example.com

ALLOYDB_PORT=5432

ALLOYDB_USER=postgres
# Change if using a different database user

ALLOYDB_PASSWORD=<DB_PASSWORD>
# Example: ALLOYDB_PASSWORD=strong_db_password_456

ALLOYDB_DATABASE=sentinel_db
# Change if using a different database name

# ------------------
# Storage Configuration
# ------------------
# Storage backend: 'gcs' for Google Cloud Storage, 'local' for filesystem
# Choose based on your infrastructure

STORAGE_BACKEND=gcs
# Options: gcs, local

# GCS Configuration (if STORAGE_BACKEND=gcs)
# ------------------------------------------
GCS_BUCKET_NAME=<YOUR_BUCKET_NAME>
# Example: GCS_BUCKET_NAME=sentinel-production-bucket

GCS_PROJECT_ID=<YOUR_PROJECT_ID>
# Example: GCS_PROJECT_ID=my-gcp-project

GCS_CREDENTIALS_PATH=/opt/sentinel_ai/credentials/gcs-key.json
# Path to your service account key file
# Make sure this file exists and has proper permissions (chmod 600)

# Local Storage Configuration (if STORAGE_BACKEND=local)
# -------------------------------------------------------
LOCAL_STORAGE_PATH=/mnt/shared_storage
# Example: LOCAL_STORAGE_PATH=/mnt/nfs/sentinel_storage
# Use this for network-mounted storage or local development

# Alternative local storage for GCS emulation
LOCAL_GCS_STORAGE_PATH=/opt/sentinel_ai/.local_gcs
# Use this for local development/testing

# ------------------
# LLM Configuration for Summarization
# ------------------
# Summary LLM generates concise document summaries
# Should point to Ollama server running a fast model (e.g., Gemma3:1b)

SUMMARY_LLM_HOST=<OLLAMA_SERVER_IP>
# Example: SUMMARY_LLM_HOST=10.0.7.10
# Example: SUMMARY_LLM_HOST=ollama.internal.example.com

SUMMARY_LLM_PORT=11434

SUMMARY_LLM_MODEL=gemma3:1b
# Model name in Ollama
# Alternatives: llama3.2:3b, mistral:7b, gemma3:1b
# Recommendation: Use a small, fast model (1b-3b parameters)

# ------------------
# LLM Configuration for Embeddings
# ------------------
# Embedding LLM creates vector representations for semantic search
# Can point to same or different Ollama server

EMBEDDING_LLM_HOST=<OLLAMA_SERVER_IP>
# Example: EMBEDDING_LLM_HOST=10.0.7.10
# Can be same as SUMMARY_LLM_HOST

EMBEDDING_LLM_PORT=11434

EMBEDDING_MODEL=embeddinggemma:latest
# Model name in Ollama
# Alternatives: nomic-embed-text, mxbai-embed-large
# Recommendation: Use embeddinggemma for consistency with summarization

# ------------------
# Document Processing Configuration
# ------------------

# Chunk size for splitting documents (in characters)
CHUNK_SIZE=2000
# Larger chunks = more context but slower processing
# Smaller chunks = faster but may lose context
# Recommendation: 1500-2500 for most documents

# Overlap between chunks (in characters)
CHUNK_OVERLAP=100
# Ensures continuity between chunks
# Recommendation: 50-200 (5-10% of CHUNK_SIZE)

# Translation threshold (in MB)
TRANSLATION_THRESHOLD_MB=10
# Documents larger than this will skip translation
# Set higher if you have powerful translation service

# Use local translation vs cloud API
TRANSLATION_LOCAL=True
# True: Use local models (slower but free)
# False: Use cloud API (faster but costs money)

# ------------------
# File Upload Configuration (Reference Only)
# ------------------
# These settings are used by the main application
# Included here for reference and consistency

MAX_UPLOAD_FILES=10
MAX_FILE_SIZE_MB=4
ALLOWED_EXTENSIONS=.pdf,.docx,.txt,.mp3,.wav,.mp4,.avi,.mov

# ------------------
# OCR Configuration
# ------------------
# Tesseract data path (set automatically by systemd service)
# Included here for reference

# TESSDATA_PREFIX=/usr/share/tesseract-ocr/5/tessdata/

# ------------------
# Development/Testing Settings
# ------------------
# Uncomment these for local development only

# USE_SQLITE_FOR_DEV=false
# SQLITE_DB_PATH=./sentinel_dev.db

# USE_GEMINI_FOR_DEV=false
# GEMINI_API_KEY=your_gemini_api_key_here

# ------------------
# Notes
# ------------------
# 1. This VM only needs access to:
#    - Redis (for queue operations)
#    - AlloyDB (for storing documents and embeddings)
#    - Storage (for reading/writing files)
#    - Summary LLM (for generating summaries)
#    - Embedding LLM (for creating embeddings)
#
# 2. This VM does NOT need:
#    - Neo4j access (graph processor handles this)
#    - Graph LLM access (graph processor handles this)
#    - Direct frontend access
#
# 3. Security:
#    - Set file permissions: chmod 600 .env
#    - Never commit this file to Git
#    - Rotate passwords regularly
#    - Use separate passwords for each environment
#
# 4. Scaling:
#    - Run multiple workers by starting additional systemd services
#    - Each worker pulls from same queue independently
#    - No coordination needed between workers
#
# 5. Troubleshooting:
#    - Check logs: /var/log/sentinel_ai/document-processor.log
#    - Test connectivity with redis-cli, psql, curl
#    - Verify Tesseract is installed: tesseract --version
#    - Check queue depth: redis-cli LLEN document_queue
