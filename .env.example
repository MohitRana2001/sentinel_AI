# ========================================
# SENTINEL AI - Environment Configuration
# ========================================
#
# Copy this file to .env for local development
# For production, set these environment variables in your deployment platform
#
# LOCAL DEV: Use Gemini API + SQLite + Local Storage
# PRODUCTION: Use Ollama/Gemma + AlloyDB + GCS
#

# ========================================
# ENVIRONMENT MODE
# ========================================
ENV=development  # Options: development, staging, production
DEBUG=True       # Set to False in production

# ========================================
# DEPLOYMENT MODE SWITCHES
# ========================================
# These flags control which services to use

# LOCAL DEV MODE (set to true for local development)
USE_GEMINI_FOR_DEV=true      # Use Gemini API for LLM tasks (dev only)
USE_SQLITE_FOR_DEV=true      # Use SQLite instead of AlloyDB (dev only)

# Set both to false for production to use Ollama/Gemma + AlloyDB + GCS

# ========================================
# API CONFIGURATION
# ========================================
API_HOST=0.0.0.0
API_PORT=8000

# CORS Origins (comma-separated)
FRONTEND_URL=http://localhost:3000
CORS_ORIGINS=http://localhost:3000,http://localhost:3001

# ========================================
# GOOGLE GEMINI API (LOCAL DEVELOPMENT ONLY)
# ========================================
# Get your API key from: https://makersuite.google.com/app/apikey
# ONLY USED WHEN USE_GEMINI_FOR_DEV=true
GEMINI_API_KEY=your-gemini-api-key-here
GOOGLE_CHAT_MODEL=gemini-2.0-flash-exp

# ========================================
# DATABASE CONFIGURATION
# ========================================

# SQLite (Local Development)
# ONLY USED WHEN USE_SQLITE_FOR_DEV=true
SQLITE_DB_PATH=./sentinel_dev.db

# AlloyDB / PostgreSQL (Production)
# These are used when USE_SQLITE_FOR_DEV=false
ALLOYDB_HOST=localhost           # Your AlloyDB instance IP or Cloud SQL Proxy
ALLOYDB_PORT=5432
ALLOYDB_USER=postgres
ALLOYDB_PASSWORD=your-alloydb-password
ALLOYDB_DATABASE=sentinel_db

# AlloyDB Connection Examples:
# Local Docker: ALLOYDB_HOST=localhost
# Cloud SQL Proxy: ALLOYDB_HOST=127.0.0.1
# Private IP: ALLOYDB_HOST=10.x.x.x
# Public IP: ALLOYDB_HOST=your-instance-ip

# ========================================
# REDIS CONFIGURATION
# ========================================
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_PASSWORD=                  # Leave empty for no auth (local dev)

# Redis channels (usually don't need to change)
# REDIS_CHANNEL_DOCUMENT=document_processor
# REDIS_CHANNEL_AUDIO=audio_processor
# REDIS_CHANNEL_VIDEO=video_processor
# REDIS_CHANNEL_GRAPH=graph_processor

# ========================================
# GOOGLE CLOUD STORAGE (GCS)
# ========================================
# For production, provide these details
GCS_BUCKET_NAME=your-gcs-bucket-name        # e.g., sentinel-ai-production
GCS_PROJECT_ID=your-gcp-project-id          # e.g., my-project-123
GCS_CREDENTIALS_PATH=/app/credentials/gcs-key.json  # Path to service account key

# For local development without GCS credentials:
# The system will automatically fall back to local storage at:
LOCAL_GCS_STORAGE_PATH=./.local_gcs

# GCS Setup Instructions:
# 1. Create a GCS bucket: gsutil mb gs://your-bucket-name
# 2. Create service account: gcloud iam service-accounts create sentinel-storage
# 3. Grant permissions: gsutil iam ch serviceAccount:sentinel-storage@PROJECT.iam.gserviceaccount.com:objectAdmin gs://your-bucket-name
# 4. Download key: gcloud iam service-accounts keys create gcs-key.json --iam-account=sentinel-storage@PROJECT.iam.gserviceaccount.com
# 5. Set GCS_CREDENTIALS_PATH to the key file location

# For Docker: mount the key file to /app/credentials/gcs-key.json
# For local: set GOOGLE_APPLICATION_CREDENTIALS=./path/to/gcs-key.json

# Alternative: Use Application Default Credentials (ADC) in GCP environments
# Set this environment variable to use ADC:
# GOOGLE_APPLICATION_CREDENTIALS=/path/to/key.json

# ========================================
# NEO4J GRAPH DATABASE
# ========================================
NEO4J_URI=bolt://localhost:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=password

# Neo4j Setup:
# Local: docker run -p 7474:7474 -p 7687:7687 -e NEO4J_AUTH=neo4j/password neo4j:5.16.0
# Production: Use Neo4j AuraDB or self-hosted instance

# ========================================
# OLLAMA / GEMMA MODELS (PRODUCTION)
# ========================================
# These are used when USE_GEMINI_FOR_DEV=false
#
# You need to run multiple Ollama instances for different models:
# - Summary LLM: Gemma3:1b (CPU) - Port 11434
# - Graph/NER LLM: Gemma3:4b (CPU) - Port 11435
# - Chat LLM: Gemma3:1b (CPU) - Port 11436
# - Multimodal LLM: Gemma3:12b (GPU) - Port 11437

# Summary LLM (for document summarization)
SUMMARY_LLM_HOST=localhost
SUMMARY_LLM_PORT=11434
SUMMARY_LLM_MODEL=gemma3:1b

# Graph/NER LLM (for entity extraction)
GRAPH_LLM_HOST=localhost
GRAPH_LLM_PORT=11435
GRAPH_LLM_MODEL=gemma3:4b

# Chat LLM (for RAG queries)
CHAT_LLM_HOST=localhost
CHAT_LLM_PORT=11436
CHAT_LLM_MODEL=gemma3:1b

# Multimodal LLM (for audio/video transcription)
MULTIMODAL_LLM_HOST=localhost
MULTIMODAL_LLM_PORT=11437
MULTIMODAL_LLM_MODEL=gemma3:12b

# Embedding Model (for vector search)
EMBEDDING_LLM_HOST=localhost
EMBEDDING_LLM_PORT=11434
EMBEDDING_MODEL=embeddinggemma:latest

# Ollama Setup Instructions:
# 1. Install Ollama: curl -fsSL https://ollama.com/install.sh | sh
# 2. Pull models:
#    ollama pull gemma3:1b
#    ollama pull gemma3:4b
#    ollama pull gemma3:12b
#    ollama pull embeddinggemma
#
# 3. Run multiple instances on different ports:
#    OLLAMA_HOST=0.0.0.0:11434 ollama serve &  # Summary
#    OLLAMA_HOST=0.0.0.0:11435 ollama serve &  # Graph
#    OLLAMA_HOST=0.0.0.0:11436 ollama serve &  # Chat
#    OLLAMA_HOST=0.0.0.0:11437 ollama serve &  # Multimodal (needs GPU)
#
# Or use Docker Compose (recommended) - see docker-compose.yml

# ========================================
# UPLOAD LIMITS
# ========================================
MAX_UPLOAD_FILES=10              # Maximum number of files per upload
MAX_FILE_SIZE_MB=4               # Maximum file size in MB
ALLOWED_EXTENSIONS=.pdf,.docx,.txt,.mp3,.wav,.mp4,.avi,.mov

# Increase these for production as needed:
# MAX_UPLOAD_FILES=50
# MAX_FILE_SIZE_MB=100

# ========================================
# TRANSLATION CONFIGURATION
# ========================================
TRANSLATION_THRESHOLD_MB=10      # File size threshold for translation
TRANSLATION_LOCAL=True           # Use local m2m100 model (production) vs Gemini (dev)

# ========================================
# SECURITY & AUTHENTICATION
# ========================================
# JWT Secret Key - CHANGE THIS IN PRODUCTION!
SECRET_KEY=your-super-secret-jwt-key-change-in-production-minimum-32-characters
ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=30

# Generate a secure secret key:
# python -c "import secrets; print(secrets.token_urlsafe(32))"

# ========================================
# TEXT PROCESSING
# ========================================
CHUNK_SIZE=2000                  # Text chunk size for embeddings
CHUNK_OVERLAP=100                # Overlap between chunks

# ========================================
# RBAC (Role-Based Access Control)
# ========================================
# Available levels: station, district, state
# These are defined in the code and don't need env vars
# Users are assigned these levels in the database

# ========================================
# GOOGLE AGENT CONFIGURATION (Optional)
# ========================================
# Comma-separated paths for additional context in chat
GOOGLE_AGENT_REFERENCE_PATHS=

# ========================================
# FRONTEND CONFIGURATION (Next.js)
# ========================================
# These should be set in your deployment platform or .env.local

NEXT_PUBLIC_API_URL=http://localhost:8000/api/v1
NEXT_PUBLIC_MAX_UPLOAD_FILES=10
NEXT_PUBLIC_MAX_FILE_SIZE_MB=4

# Production Frontend:
# NEXT_PUBLIC_API_URL=https://your-backend-url.com/api/v1

# ========================================
# DOCKER DEPLOYMENT
# ========================================
# If using Docker Compose, most of these are set automatically
# Just set these in your .env file:
#
# For LOCAL development with Docker:
# USE_GEMINI_FOR_DEV=true
# GEMINI_API_KEY=your-key
#
# For PRODUCTION with Docker:
# USE_GEMINI_FOR_DEV=false
# GCS_BUCKET_NAME=your-bucket
# GCS_PROJECT_ID=your-project
# ALLOYDB_HOST=your-alloydb-ip
# ALLOYDB_PASSWORD=your-password

# ========================================
# QUICK START GUIDE
# ========================================
#
# LOCAL DEVELOPMENT (Gemini + SQLite + Local Storage):
# 1. Copy this file: cp .env.example .env
# 2. Set: USE_GEMINI_FOR_DEV=true
# 3. Set: USE_SQLITE_FOR_DEV=true
# 4. Add your GEMINI_API_KEY
# 5. Run: docker-compose up -d redis neo4j
# 6. Run: cd backend && python main.py
# 7. Run: npm run dev (in root)
#
# PRODUCTION (Ollama + AlloyDB + GCS):
# 1. Set: USE_GEMINI_FOR_DEV=false
# 2. Set: USE_SQLITE_FOR_DEV=false
# 3. Configure AlloyDB credentials
# 4. Configure GCS credentials
# 5. Set up Ollama instances with Gemma models
# 6. Deploy using Docker Compose or Kubernetes
#
# TOGGLE BETWEEN ENVIRONMENTS:
# Just change USE_GEMINI_FOR_DEV and USE_SQLITE_FOR_DEV flags!
#
# ========================================
