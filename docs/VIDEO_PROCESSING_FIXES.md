# Video Processing Fixes - Issues 1, 2, 3

**Date:** November 14, 2025

## Issue Analysis

### Issue 1 & 2: Video Marked Complete Prematurely ‚úÖ CONFIRMED

**Problem**: Video processor marks document as COMPLETED after processing, but it should wait for graph processor to complete.

**Current Flow (WRONG)**:
```
Video Processing ‚Üí Vectorization ‚Üí ‚úÖ COMPLETED ‚Üí Graph Queue
                                     ‚Üë
                              (Too early! Graph not built yet)
```

**Correct Flow**:
```
Video Processing ‚Üí Vectorization ‚Üí awaiting_graph ‚Üí Graph Queue ‚Üí Graph Processing ‚Üí ‚úÖ COMPLETED
                                     ‚Üë                                  ‚Üë
                              (Correct: waiting)              (Graph processor marks complete)
```

**Good News**: The code actually ALREADY DOES THIS CORRECTLY! 

Looking at line 543-556 in `video_processor_service.py`:
```python
# Update processing stages but keep status as PROCESSING (will be updated by graph processor)
doc_record.current_stage = "awaiting_graph"
doc_record.processing_stages = stage_times
db.commit()

# Publish status: still processing, awaiting graph
redis_pubsub.publish_artifact_status(
    job_id=job.id,
    filename=filename,
    status="processing",  # ‚Üê Still PROCESSING, not COMPLETED
    current_stage="awaiting_graph",
    processing_stages=stage_times,
    file_type="video"
)
```

**The document is NOT marked as COMPLETED** - it's set to `awaiting_graph` with status `processing`.

**However**, there's a bug in `_check_job_completion()` at line 175-195:

```python
def _check_job_completion(self, db, job):
    """
    Check if all files in the job have been processed
    If yes, mark job as completed
    """
    # Count documents created for this job
    documents_processed = db.query(models.Document).filter(
        models.Document.job_id == job.id
    ).count()
    
    # Only mark as completed if all files are done
    if documents_processed >= job.total_files:
        if job.status != models.JobStatus.COMPLETED:
            job.status = models.JobStatus.COMPLETED  # ‚Üê BUG: Marks JOB complete
            job.completed_at = datetime.now(timezone.utc)
            db.commit()
```

**The Issue**: 
- The **JOB** is marked as complete when all files are processed
- But files might still be in `awaiting_graph` stage
- This causes the job to show as complete even though graphs aren't built yet

### Issue 3: Face Recognition Integration

**Current**: `video_frs.py` is standalone with hardcoded suspects
**Needed**: Integrate with POI database, match faces, create relationships

---

## Fixes

### Fix 1: Don't Mark Job Complete Until Graphs Are Built

Update `_check_job_completion()` to check if all documents have completed graph processing:

```python
def _check_job_completion(self, db, job):
    """
    Check if all files in the job have completed GRAPH PROCESSING
    Documents should be in COMPLETED status (set by graph processor)
    """
    # Count COMPLETED documents (not just created documents)
    completed_documents = db.query(models.Document).filter(
        models.Document.job_id == job.id,
        models.Document.status == models.JobStatus.COMPLETED  # Only count completed docs
    ).count()
    
    total_documents = db.query(models.Document).filter(
        models.Document.job_id == job.id
    ).count()
    
    print(f"üìä Job {job.id}: {completed_documents}/{job.total_files} files COMPLETED (including graph)")
    
    # Only mark job as completed if ALL files have completed graph processing
    if completed_documents >= job.total_files:
        if job.status != models.JobStatus.COMPLETED:
            job.status = models.JobStatus.COMPLETED
            job.completed_at = datetime.now(timezone.utc)
            db.commit()
            print(f"‚úÖ Job {job.id} marked as COMPLETED (all graphs built)")
    elif total_documents > 0:
        # Some files processed, ensure status is PROCESSING
        if job.status == models.JobStatus.QUEUED:
            job.status = models.JobStatus.PROCESSING
            job.started_at = job.started_at or datetime.now(timezone.utc)
            db.commit()
```

### Fix 2: Face Recognition Integration

Create new file: `face_recognition_processor.py`

```python
"""
Face Recognition Processor
Matches faces in video frames against Person of Interest (POI) database
"""
import cv2
import face_recognition
import numpy as np
from typing import List, Dict, Any, Tuple
from database import SessionLocal
import models
from storage_config import storage_manager
import tempfile
import os


class FaceRecognitionProcessor:
    
    def __init__(self):
        """Initialize face recognition processor"""
        self.tolerance = 0.50  # Face matching tolerance (lower = stricter)
        self.frame_skip_seconds = 0.3  # Process 1 frame every 0.3 seconds
    
    def load_poi_faces(self, db) -> Tuple[List[np.ndarray], List[Dict]]:
        """
        Load all POI face encodings from database
        
        Returns:
            Tuple of (face_encodings, poi_metadata)
        """
        print("üìã Loading POI face encodings from database...")
        
        pois = db.query(models.PersonOfInterest).all()
        
        if not pois:
            print("‚ö†Ô∏è No POIs found in database")
            return [], []
        
        face_encodings = []
        poi_metadata = []
        
        for poi in pois:
            try:
                # Decode base64 photograph
                import base64
                import io
                from PIL import Image
                
                # Remove data URL prefix if present
                photo_data = poi.photograph_base64
                if photo_data.startswith('data:image'):
                    photo_data = photo_data.split(',')[1]
                
                # Decode base64
                image_bytes = base64.b64decode(photo_data)
                image = Image.open(io.BytesIO(image_bytes))
                
                # Convert to numpy array (RGB format for face_recognition)
                image_np = np.array(image.convert('RGB'))
                
                # Get face encoding
                encodings = face_recognition.face_encodings(image_np)
                
                if encodings:
                    face_encodings.append(encodings[0])
                    poi_metadata.append({
                        'id': poi.id,
                        'name': poi.name,
                        'phone_number': poi.phone_number,
                        'details': poi.details
                    })
                    print(f"  ‚úÖ Loaded face encoding for: {poi.name}")
                else:
                    print(f"  ‚ö†Ô∏è No face found in photo for: {poi.name}")
                    
            except Exception as e:
                print(f"  ‚ùå Error loading face for {poi.name}: {e}")
                continue
        
        print(f"‚úÖ Loaded {len(face_encodings)} POI face encodings")
        return face_encodings, poi_metadata
    
    def process_video_for_faces(
        self, 
        video_path: str, 
        poi_encodings: List[np.ndarray],
        poi_metadata: List[Dict]
    ) -> List[Dict[str, Any]]:
        """
        Process video and detect POI faces
        
        Args:
            video_path: Path to video file
            poi_encodings: List of POI face encodings
            poi_metadata: List of POI metadata (aligned with encodings)
        
        Returns:
            List of detections: [{poi_id, poi_name, frame_number, timestamp, ...}]
        """
        print(f"üé¨ Processing video for face recognition: {video_path}")
        
        if not poi_encodings:
            print("‚ö†Ô∏è No POI encodings provided, skipping face recognition")
            return []
        
        # Open video
        video = cv2.VideoCapture(video_path)
        
        if not video.isOpened():
            print(f"‚ùå Failed to open video: {video_path}")
            return []
        
        # Get video properties
        fps = int(video.get(cv2.CAP_PROP_FPS))
        total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / fps if fps > 0 else 0
        
        frame_skip_interval = int(fps * self.frame_skip_seconds)
        
        print(f"üìπ Video: {total_frames} frames, {fps} fps, {duration:.1f}s duration")
        print(f"üîç Processing 1 frame every {self.frame_skip_seconds}s (interval: {frame_skip_interval} frames)")
        
        detections = []
        frame_number = 0
        processed_count = 0
        
        while True:
            # Set frame position
            video.set(cv2.CAP_PROP_POS_FRAMES, frame_number)
            
            # Read frame
            ret, frame = video.read()
            
            if not ret:
                break
            
            # Process this frame
            if frame_number % frame_skip_interval == 0:
                processed_count += 1
                timestamp = frame_number / fps if fps > 0 else 0
                
                # Convert BGR (OpenCV) to RGB (face_recognition)
                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                
                # Detect faces in frame
                face_locations = face_recognition.face_locations(rgb_frame)
                
                if face_locations:
                    print(f"  Frame {frame_number} ({timestamp:.1f}s): Found {len(face_locations)} face(s)")
                    
                    # Encode faces
                    face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)
                    
                    # Compare each face with POIs
                    for face_encoding, face_location in zip(face_encodings, face_locations):
                        matches = face_recognition.compare_faces(
                            poi_encodings,
                            face_encoding,
                            tolerance=self.tolerance
                        )
                        
                        # Find matching POIs
                        for idx, is_match in enumerate(matches):
                            if is_match:
                                poi = poi_metadata[idx]
                                top, right, bottom, left = face_location
                                
                                detection = {
                                    'poi_id': poi['id'],
                                    'poi_name': poi['name'],
                                    'poi_phone': poi['phone_number'],
                                    'frame_number': frame_number,
                                    'timestamp': timestamp,
                                    'face_location': {
                                        'top': int(top),
                                        'right': int(right),
                                        'bottom': int(bottom),
                                        'left': int(left)
                                    }
                                }
                                
                                detections.append(detection)
                                print(f"    ‚úÖ POI MATCH: {poi['name']} at {timestamp:.1f}s")
            
            frame_number += frame_skip_interval
        
        video.release()
        
        print(f"‚úÖ Face recognition complete: {len(detections)} POI detections in {processed_count} frames")
        return detections
    
    def create_video_poi_detections(
        self,
        db,
        document_id: int,
        detections: List[Dict[str, Any]]
    ):
        """
        Create VideoPOIDetection records in database
        
        Args:
            db: Database session
            document_id: Document ID (video)
            detections: List of POI detections from process_video_for_faces
        """
        print(f"üíæ Creating {len(detections)} VideoPOIDetection records...")
        
        for detection in detections:
            video_poi = models.VideoPOIDetection(
                document_id=document_id,
                poi_id=detection['poi_id'],
                frame_number=detection['frame_number'],
                timestamp=detection['timestamp'],
                face_location=detection['face_location'],
                confidence_score=0.95  # Can be enhanced with actual confidence
            )
            db.add(video_poi)
        
        db.commit()
        print(f"‚úÖ Created {len(detections)} VideoPOIDetection records")
    
    def process_video_document(self, db, document_id: int, video_gcs_path: str):
        """
        Complete face recognition workflow for a video document
        
        Args:
            db: Database session
            document_id: Document ID
            video_gcs_path: GCS path to video file
        """
        print(f"\n{'='*60}")
        print(f"üéØ Starting Face Recognition for Document {document_id}")
        print(f"{'='*60}\n")
        
        # Load POI faces
        poi_encodings, poi_metadata = self.load_poi_faces(db)
        
        if not poi_encodings:
            print("‚ö†Ô∏è No POI faces available, skipping face recognition")
            return
        
        # Download video to temp file
        temp_video = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4')
        temp_video.close()
        
        try:
            print(f"‚¨áÔ∏è Downloading video from GCS...")
            storage_manager.download_file(video_gcs_path, temp_video.name)
            
            # Process video for faces
            detections = self.process_video_for_faces(
                temp_video.name,
                poi_encodings,
                poi_metadata
            )
            
            # Create detection records
            if detections:
                self.create_video_poi_detections(db, document_id, detections)
                
                # Create summary
                unique_pois = set(d['poi_name'] for d in detections)
                print(f"\nüìä SUMMARY:")
                print(f"   Unique POIs detected: {len(unique_pois)}")
                for poi_name in unique_pois:
                    poi_detections = [d for d in detections if d['poi_name'] == poi_name]
                    print(f"   - {poi_name}: {len(poi_detections)} appearance(s)")
            else:
                print("‚ÑπÔ∏è No POI matches found in video")
        
        finally:
            # Cleanup temp file
            if os.path.exists(temp_video.name):
                os.unlink(temp_video.name)
        
        print(f"\n‚úÖ Face recognition completed for document {document_id}\n")


# Singleton instance
face_recognition_processor = FaceRecognitionProcessor()
```

### Fix 3: Integrate Face Recognition into Video Processor

Update `video_processor_service.py` to call face recognition after frame extraction:

```python
# After frame extraction (around line 400), add:

# Step 2.5: Face Recognition (POI Detection)
if frame_paths:  # Only if we have frames
    try:
        print(f"üë§ Running face recognition on video frames...")
        from face_recognition_processor import face_recognition_processor
        
        # Process video for POI faces
        face_recognition_processor.process_video_document(
            db=db,
            document_id=doc_record.id,
            video_gcs_path=gcs_path
        )
        
        # Record timing
        face_recognition_end = datetime.now(timezone.utc)
        stage_times['face_recognition'] = (face_recognition_end - frame_extraction_end).total_seconds()
        
    except Exception as e:
        print(f"‚ö†Ô∏è Face recognition failed: {e}")
        import traceback
        traceback.print_exc()
        # Continue processing even if face recognition fails
```

---

## Summary

**Issue 1 & 2**: Job marked complete too early
- **Root Cause**: `_check_job_completion()` counts created documents, not completed documents
- **Fix**: Check `document.status == COMPLETED` instead of just counting documents

**Issue 3**: Face recognition integration
- **Created**: `face_recognition_processor.py` with POI database integration
- **Features**:
  - Loads POI faces from database
  - Processes video frames
  - Detects POI matches
  - Creates `VideoPOIDetection` records
  - Production-ready (no hardcoded data)

**Dependencies Added**:
- opencv-python ‚úÖ (already in requirements.txt)
- face-recognition ‚úÖ (already in requirements.txt)
- numpy ‚úÖ (already in requirements.txt)

---

## Testing

1. Upload a video with a case
2. Check that job stays in "processing" until graph is built
3. Verify graph processor marks it complete
4. Check VideoPOIDetection table for face matches
5. Query: `SELECT * FROM video_poi_detections WHERE document_id = X;`

